{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.12"},"colab":{"name":"Experiment_3.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"08ta3u-ywqdy","executionInfo":{"status":"ok","timestamp":1599324661474,"user_tz":-480,"elapsed":4199,"user":{"displayName":"zhenglin liu","photoUrl":"","userId":"11338737901549158720"}},"outputId":"e396f27f-6b09-4d23-ad22-936c1b0b5825","colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["# !pip install deepdish\n","!pip install tf_slim"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tf_slim in /usr/local/lib/python3.6/dist-packages (1.1.0)\n","Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y3aWrvvAqP-N","executionInfo":{"status":"ok","timestamp":1601399333830,"user_tz":-480,"elapsed":28654,"user":{"displayName":"zhenglin liu","photoUrl":"","userId":"11338737901549158720"}},"outputId":"4cf48665-64f2-4173-eb9f-cb0351e0a7d4","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PAJwGQ4Kwn3p"},"source":["import sys\n","sys.path.append('./drive/My Drive/project/code')\n","sys.path\n","from function_2 import *  # for input size 512\n","from function_1 import *  # for input size 128\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# import deepdish as dd\n","import tensorflow as tf\n","import tf_slim as slim\n","import os\n","tf.compat.v1.disable_eager_execution()\n","# tf.config.set_soft_device_placement(True)\n","# tf.config.experimental.list_physical_devices('GPU')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5weo8WKwn39"},"source":["# This parameter setting is referred from https://github.com/robertocalandra/the-feeling-of-success/tree/master/manu_sawyer/src/tensorflow_model_is_gripping\n","parm = gel_fulldata_v5()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sW15dbBJwn4J","executionInfo":{"status":"ok","timestamp":1599326441076,"user_tz":-480,"elapsed":1745292,"user":{"displayName":"zhenglin liu","photoUrl":"","userId":"11338737901549158720"}},"outputId":"de2528d5-eb49-497f-fa58-2bb3555caeec","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["init_path='./drive/My Drive/project/vgg_16.ckpt'\n","os.putenv('CUDA_VISIBLE_DEVICES', ','.join(map(str, [0,1])))\n","mkdir(parm.resdir)\n","mkdir(parm.dsdir)\n","mkdir(parm.train_dir)\n","use_reg = True\n","restore = False\n","update_top_only=False\n","tf.compat.v1.reset_default_graph()\n","config = tf.compat.v1.ConfigProto(allow_soft_placement = True)\n","tf.Graph().as_default()\n","# with tf.Graph().as_default(), tf.device('/cpu:0'), tf.compat.v1.Session(config = config) as sess:\n","with tf.Graph().as_default(), tf.device('/device:GPU:0'), tf.compat.v1.Session(config = config) as sess:\n","\n","    inputs = read_data(parm,sess)\n","    global_step = tf.compat.v1.get_variable('global_step', [], initializer = \n","                              tf.constant_initializer(0), trainable = False)\n","\n","    lr = parm.base_lr * parm.lr_gamma**(global_step // parm.step_size)\n","\n","    if parm.opt_method == 'adam':\n","      opt = tf.compat.v1.train.AdamOptimizer(lr)\n","    elif parm.opt_method == 'momentum':\n","      opt = tf.compat.v1.train.MomentumOptimizer(lr, 0.9)\n","\n","    with tf.device('/device:GPU:0'):\n","      label = inputs['is_gripping']\n","\n","      logits = make_model(inputs, parm, update_top_only = update_top_only, train = True, reuse = False)\n","      loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n","        logits = logits, labels = label)\n","      loss = tf.reduce_mean(loss)\n","      if use_reg:\n","        reg_losses = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)\n","        print ('Number of regularization losses:', len(reg_losses))\n","        loss = loss + tf.add_n(reg_losses)\n","      eq = tf.equal(tf.argmax(logits, 1), label)\n","      acc = tf.reduce_mean(tf.cast(eq, tf.float32))\n","\n","    train_op = opt.minimize(loss, global_step = global_step)\n","\n","    bn_ups = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n","    print ('Batch norm updates:', len(bn_ups))\n","    train_op = tf.group(train_op, *bn_ups)\n","\n","    sess.run(tf.compat.v1.global_variables_initializer())\n","    var_list = slim.get_variables_to_restore()\n","    re=var_list\n","\n","    if restore:\n","      exclude = ['Adam', 'beta1_power', 'beta2_power', 'Momentum', 'global_step' , 'fc6_']\n","    else:\n","      exclude = ['Adam', 'beta1_power', 'beta2_power', 'Momentum', 'global_step', 'logits', 'fc8', 'fc6_', 'fc7_', 'conv6']\n","\n","    var_list = [x for x in var_list if \\\n","                not any(name in x.name for name in exclude)]\n","\n","    if restore:\n","      tf.compat.v1.train.Saver(var_list).restore(sess, pj('./drive/My Drive/project/result/gel-fulldata-v5/training', 'net.tf-set123'))\n","      # tf.compat.v1.train.Saver(var_list).restore(sess, tf.train.latest_checkpoint(parm.train_dir))\n","    else:\n","      for base in ['im', 'depth', 'gel']:\n","          print ('Restoring:', base)\n","          mapping = {}\n","          for v in var_list:\n","            start = '%s_vgg16/' % base\n","            if v.name.startswith(start):\n","              vgg_name = v.name.replace(start, 'vgg_16/')\n","              vgg_name = vgg_name[:-2]\n","              print (vgg_name, '->', v.name)\n","              mapping[vgg_name] = v\n","          if len(mapping):\n","            tf.compat.v1.train.Saver(mapping).restore(sess, init_path)\n","\n","    tf.compat.v1.train.start_queue_runners(sess = sess)\n","    for i in range(parm.train_iters):\n","      step = int(sess.run(global_step))\n","      # print(step)\n","      # if (step == 10 or step % 100 == 0) or step == parm.train_iters - 1:\n","      if step % 1000 == 0 or step == parm.train_iters - 1:\n","        check_path = os.path.join(mkdir(parm.train_dir), 'net.tf')\n","        print ('Saving:', check_path)\n","        vs = slim.get_model_variables()\n","        tf.compat.v1.train.Saver(vs).save(sess, check_path, global_step = global_step)\n","      if step > parm.train_iters:\n","        break\n","\n","      _, lr_val, loss_val, acc_val = sess.run([train_op, lr, loss, acc])\n","\n","      if step % 10 == 0:\n","        print ('Iteration %d,' % step, 'lr = ', lr_val, \\\n","              'loss:', moving_avg('loss', loss_val), 'acc:', moving_avg('acc', acc_val))\n","        sys.stdout.flush()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tf files: ['./drive/My Drive/project/result/train128_set2.tf', './drive/My Drive/project/result/train128_set3.tf', './drive/My Drive/project/result/train128_set4.tf']\n","WARNING:tensorflow:From ./drive/My Drive/project/code/function_2.py:421: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:112: BaseResourceVariable.count_up_to (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Prefer Dataset.range instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","WARNING:tensorflow:From ./drive/My Drive/project/code/function_2.py:364: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n","group:\n","gelsightA_before (112, 112, 3)\n","gelsightA_during (112, 112, 3)\n","gelsightB_before (112, 112, 3)\n","gelsightB_during (112, 112, 3)\n","WARNING:tensorflow:From ./drive/My Drive/project/code/function_2.py:426: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n","reuse = False\n","False\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","True\n","False\n","True\n","Number of regularization losses: 16\n","Batch norm updates: 0\n","Restoring: im\n","Restoring: depth\n","Restoring: gel\n","vgg_16/conv1/conv1_1/weights -> gel_vgg16/conv1/conv1_1/weights:0\n","vgg_16/conv1/conv1_1/biases -> gel_vgg16/conv1/conv1_1/biases:0\n","vgg_16/conv1/conv1_2/weights -> gel_vgg16/conv1/conv1_2/weights:0\n","vgg_16/conv1/conv1_2/biases -> gel_vgg16/conv1/conv1_2/biases:0\n","vgg_16/conv2/conv2_1/weights -> gel_vgg16/conv2/conv2_1/weights:0\n","vgg_16/conv2/conv2_1/biases -> gel_vgg16/conv2/conv2_1/biases:0\n","vgg_16/conv2/conv2_2/weights -> gel_vgg16/conv2/conv2_2/weights:0\n","vgg_16/conv2/conv2_2/biases -> gel_vgg16/conv2/conv2_2/biases:0\n","vgg_16/conv3/conv3_1/weights -> gel_vgg16/conv3/conv3_1/weights:0\n","vgg_16/conv3/conv3_1/biases -> gel_vgg16/conv3/conv3_1/biases:0\n","vgg_16/conv3/conv3_2/weights -> gel_vgg16/conv3/conv3_2/weights:0\n","vgg_16/conv3/conv3_2/biases -> gel_vgg16/conv3/conv3_2/biases:0\n","vgg_16/conv3/conv3_3/weights -> gel_vgg16/conv3/conv3_3/weights:0\n","vgg_16/conv3/conv3_3/biases -> gel_vgg16/conv3/conv3_3/biases:0\n","vgg_16/conv4/conv4_1/weights -> gel_vgg16/conv4/conv4_1/weights:0\n","vgg_16/conv4/conv4_1/biases -> gel_vgg16/conv4/conv4_1/biases:0\n","vgg_16/conv4/conv4_2/weights -> gel_vgg16/conv4/conv4_2/weights:0\n","vgg_16/conv4/conv4_2/biases -> gel_vgg16/conv4/conv4_2/biases:0\n","vgg_16/conv4/conv4_3/weights -> gel_vgg16/conv4/conv4_3/weights:0\n","vgg_16/conv4/conv4_3/biases -> gel_vgg16/conv4/conv4_3/biases:0\n","vgg_16/conv5/conv5_1/weights -> gel_vgg16/conv5/conv5_1/weights:0\n","vgg_16/conv5/conv5_1/biases -> gel_vgg16/conv5/conv5_1/biases:0\n","vgg_16/conv5/conv5_2/weights -> gel_vgg16/conv5/conv5_2/weights:0\n","vgg_16/conv5/conv5_2/biases -> gel_vgg16/conv5/conv5_2/biases:0\n","vgg_16/conv5/conv5_3/weights -> gel_vgg16/conv5/conv5_3/weights:0\n","vgg_16/conv5/conv5_3/biases -> gel_vgg16/conv5/conv5_3/biases:0\n","INFO:tensorflow:Restoring parameters from ./drive/My Drive/project/vgg_16.ckpt\n","WARNING:tensorflow:From <ipython-input-8-b7d7ba62e25a>:74: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","To construct input pipelines, use the `tf.data` module.\n","Saving: ./drive/My Drive/project/result/gel-128-v5/training/net.tf\n","Iteration 0, lr =  1e-05 loss: 2.594658 acc: 0.625\n","Iteration 10, lr =  1e-05 loss: 2.388995 acc: 0.78125\n","Iteration 20, lr =  1e-05 loss: 2.4869297 acc: 0.75\n","Iteration 30, lr =  1e-05 loss: 2.387139 acc: 0.8125\n","Iteration 40, lr =  1e-05 loss: 2.365418 acc: 0.8125\n","Iteration 50, lr =  1e-05 loss: 2.3374798 acc: 0.8333333\n","Iteration 60, lr =  1e-05 loss: 2.321205 acc: 0.8214286\n","Iteration 70, lr =  1e-05 loss: 2.3437545 acc: 0.8125\n","Iteration 80, lr =  1e-05 loss: 2.3154268 acc: 0.8194444\n","Iteration 90, lr =  1e-05 loss: 2.2896469 acc: 0.83125\n","Iteration 100, lr =  1e-05 loss: 2.2763813 acc: 0.83522725\n","Iteration 110, lr =  1e-05 loss: 2.2751281 acc: 0.8229167\n","Iteration 120, lr =  1e-05 loss: 2.27513 acc: 0.82211536\n","Iteration 130, lr =  1e-05 loss: 2.2737155 acc: 0.8214286\n","Iteration 140, lr =  1e-05 loss: 2.2761981 acc: 0.8125\n","Iteration 150, lr =  1e-05 loss: 2.2669804 acc: 0.81640625\n","Iteration 160, lr =  1e-05 loss: 2.2695448 acc: 0.8014706\n","Iteration 170, lr =  1e-05 loss: 2.2796702 acc: 0.7847222\n","Iteration 180, lr =  1e-05 loss: 2.2814507 acc: 0.77960527\n","Iteration 190, lr =  1e-05 loss: 2.277047 acc: 0.778125\n","Iteration 200, lr =  1e-05 loss: 2.2667203 acc: 0.78571427\n","Iteration 210, lr =  1e-05 loss: 2.2785237 acc: 0.7755682\n","Iteration 220, lr =  1e-05 loss: 2.2647 acc: 0.78532606\n","Iteration 230, lr =  1e-05 loss: 2.2570674 acc: 0.78125\n","Iteration 240, lr =  1e-05 loss: 2.245736 acc: 0.785\n","Iteration 250, lr =  1e-05 loss: 2.2305512 acc: 0.7932692\n","Iteration 260, lr =  1e-05 loss: 2.2298846 acc: 0.7916667\n","Iteration 270, lr =  1e-05 loss: 2.222617 acc: 0.79464287\n","Iteration 280, lr =  1e-05 loss: 2.2131069 acc: 0.79741377\n","Iteration 290, lr =  1e-05 loss: 2.2128863 acc: 0.79791665\n","Iteration 300, lr =  1e-05 loss: 2.216433 acc: 0.7983871\n","Iteration 310, lr =  1e-05 loss: 2.2060785 acc: 0.80078125\n","Iteration 320, lr =  1e-05 loss: 2.1996145 acc: 0.8011364\n","Iteration 330, lr =  1e-05 loss: 2.1841674 acc: 0.8069853\n","Iteration 340, lr =  1e-05 loss: 2.176673 acc: 0.80892855\n","Iteration 350, lr =  1e-05 loss: 2.173973 acc: 0.8090278\n","Iteration 360, lr =  1e-05 loss: 2.1835647 acc: 0.8006757\n","Iteration 370, lr =  1e-05 loss: 2.1782742 acc: 0.8009868\n","Iteration 380, lr =  1e-05 loss: 2.1712673 acc: 0.80128205\n","Iteration 390, lr =  1e-05 loss: 2.1740284 acc: 0.7921875\n","Iteration 400, lr =  1e-05 loss: 2.1681306 acc: 0.79420733\n","Iteration 410, lr =  1e-05 loss: 2.158158 acc: 0.79761904\n","Iteration 420, lr =  1e-05 loss: 2.1488693 acc: 0.8008721\n","Iteration 430, lr =  1e-05 loss: 2.1420496 acc: 0.8025568\n","Iteration 440, lr =  1e-05 loss: 2.1376688 acc: 0.8041667\n","Iteration 450, lr =  1e-05 loss: 2.1308863 acc: 0.8057065\n","Iteration 460, lr =  1e-05 loss: 2.1261997 acc: 0.8071808\n","Iteration 470, lr =  1e-05 loss: 2.1187575 acc: 0.8098958\n","Iteration 480, lr =  1e-05 loss: 2.1148503 acc: 0.81122446\n","Iteration 490, lr =  1e-05 loss: 2.1147428 acc: 0.81\n","Iteration 500, lr =  1e-05 loss: 2.1145434 acc: 0.8088235\n","Iteration 510, lr =  1e-05 loss: 2.11099 acc: 0.8088942\n","Iteration 520, lr =  1e-05 loss: 2.107012 acc: 0.8089623\n","Iteration 530, lr =  1e-05 loss: 2.0998132 acc: 0.8113426\n","Iteration 540, lr =  1e-05 loss: 2.0947528 acc: 0.8125\n","Iteration 550, lr =  1e-05 loss: 2.089478 acc: 0.8125\n","Iteration 560, lr =  1e-05 loss: 2.0829341 acc: 0.8135965\n","Iteration 570, lr =  1e-05 loss: 2.079032 acc: 0.8135776\n","Iteration 580, lr =  1e-05 loss: 2.0805266 acc: 0.81038135\n","Iteration 590, lr =  1e-05 loss: 2.0804937 acc: 0.809375\n","Iteration 600, lr =  1e-05 loss: 2.081448 acc: 0.80942625\n","Iteration 610, lr =  1e-05 loss: 2.0751994 acc: 0.8114919\n","Iteration 620, lr =  1e-05 loss: 2.0696902 acc: 0.81349206\n","Iteration 630, lr =  1e-05 loss: 2.065408 acc: 0.8154297\n","Iteration 640, lr =  1e-05 loss: 2.0624146 acc: 0.81634617\n","Iteration 650, lr =  1e-05 loss: 2.0606675 acc: 0.8162879\n","Iteration 660, lr =  1e-05 loss: 2.0575867 acc: 0.8171642\n","Iteration 670, lr =  1e-05 loss: 2.054181 acc: 0.8170956\n","Iteration 680, lr =  1e-05 loss: 2.0503669 acc: 0.8179348\n","Iteration 690, lr =  1e-05 loss: 2.052084 acc: 0.8151786\n","Iteration 700, lr =  1e-05 loss: 2.0487113 acc: 0.81514084\n","Iteration 710, lr =  1e-05 loss: 2.0469255 acc: 0.8142361\n","Iteration 720, lr =  1e-05 loss: 2.0435472 acc: 0.8150685\n","Iteration 730, lr =  1e-05 loss: 2.03997 acc: 0.8150338\n","Iteration 740, lr =  1e-05 loss: 2.034845 acc: 0.81666666\n","Iteration 750, lr =  1e-05 loss: 2.0300877 acc: 0.81825656\n","Iteration 760, lr =  1e-05 loss: 2.027383 acc: 0.8181818\n","Iteration 770, lr =  1e-05 loss: 2.0238528 acc: 0.818109\n","Iteration 780, lr =  1e-05 loss: 2.0188215 acc: 0.8188291\n","Iteration 790, lr =  1e-05 loss: 2.0124118 acc: 0.82109374\n","Iteration 800, lr =  1e-05 loss: 2.0070877 acc: 0.82330245\n","Iteration 810, lr =  1e-05 loss: 2.0030966 acc: 0.8246951\n","Iteration 820, lr =  1e-05 loss: 1.9985931 acc: 0.8260542\n","Iteration 830, lr =  1e-05 loss: 1.9960469 acc: 0.8266369\n","Iteration 840, lr =  1e-05 loss: 1.9919505 acc: 0.8279412\n","Iteration 850, lr =  1e-05 loss: 1.9908828 acc: 0.82776165\n","Iteration 860, lr =  1e-05 loss: 1.9894557 acc: 0.82758623\n","Iteration 870, lr =  1e-05 loss: 1.9911748 acc: 0.8259943\n","Iteration 880, lr =  1e-05 loss: 1.9877172 acc: 0.8272472\n","Iteration 890, lr =  1e-05 loss: 1.987532 acc: 0.82569444\n","Iteration 900, lr =  1e-05 loss: 1.98561 acc: 0.82623625\n","Iteration 910, lr =  1e-05 loss: 1.984995 acc: 0.82472825\n","Iteration 920, lr =  1e-05 loss: 1.9852468 acc: 0.82258064\n","Iteration 930, lr =  1e-05 loss: 1.9835082 acc: 0.8231383\n","Iteration 940, lr =  1e-05 loss: 1.9826978 acc: 0.8217105\n","Iteration 950, lr =  1e-05 loss: 1.981679 acc: 0.82096356\n","Iteration 960, lr =  1e-05 loss: 1.9813098 acc: 0.820232\n","Iteration 970, lr =  1e-05 loss: 1.9794754 acc: 0.8207908\n","Iteration 980, lr =  1e-05 loss: 1.9788741 acc: 0.8194444\n","Iteration 990, lr =  1e-05 loss: 1.9830002 acc: 0.8175\n","Saving: ./drive/My Drive/project/result/gel-128-v5/training/net.tf\n","Iteration 1000, lr =  1e-05 loss: 1.9767351 acc: 0.81875\n","Iteration 1010, lr =  1e-05 loss: 1.9727924 acc: 0.818125\n","Iteration 1020, lr =  1e-05 loss: 1.9624815 acc: 0.820625\n","Iteration 1030, lr =  1e-05 loss: 1.9597322 acc: 0.819375\n","Iteration 1040, lr =  1e-05 loss: 1.9519289 acc: 0.82125\n","Iteration 1050, lr =  1e-05 loss: 1.9454429 acc: 0.82125\n","Iteration 1060, lr =  1e-05 loss: 1.9425503 acc: 0.820625\n","Iteration 1070, lr =  1e-05 loss: 1.9331568 acc: 0.823125\n","Iteration 1080, lr =  1e-05 loss: 1.9297825 acc: 0.8225\n","Iteration 1090, lr =  1e-05 loss: 1.9249392 acc: 0.8225\n","Iteration 1100, lr =  1e-05 loss: 1.9194468 acc: 0.823125\n","Iteration 1110, lr =  1e-05 loss: 1.9131279 acc: 0.825\n","Iteration 1120, lr =  1e-05 loss: 1.9063809 acc: 0.825625\n","Iteration 1130, lr =  1e-05 loss: 1.899052 acc: 0.826875\n","Iteration 1140, lr =  1e-05 loss: 1.8926864 acc: 0.82875\n","Iteration 1150, lr =  1e-05 loss: 1.8917079 acc: 0.825\n","Iteration 1160, lr =  1e-05 loss: 1.8851665 acc: 0.828125\n","Iteration 1170, lr =  1e-05 loss: 1.8768103 acc: 0.8325\n","Iteration 1180, lr =  1e-05 loss: 1.8749272 acc: 0.833125\n","Iteration 1190, lr =  1e-05 loss: 1.873388 acc: 0.83125\n","Iteration 1200, lr =  1e-05 loss: 1.87081 acc: 0.83\n","Iteration 1210, lr =  1e-05 loss: 1.8612454 acc: 0.83375\n","Iteration 1220, lr =  1e-05 loss: 1.8576971 acc: 0.8325\n","Iteration 1230, lr =  1e-05 loss: 1.8529354 acc: 0.835\n","Iteration 1240, lr =  1e-05 loss: 1.8506289 acc: 0.834375\n","Iteration 1250, lr =  1e-05 loss: 1.8486814 acc: 0.833125\n","Iteration 1260, lr =  1e-05 loss: 1.8413632 acc: 0.835\n","Iteration 1270, lr =  1e-05 loss: 1.8371638 acc: 0.834375\n","Iteration 1280, lr =  1e-05 loss: 1.8348209 acc: 0.835\n","Iteration 1290, lr =  1e-05 loss: 1.8280939 acc: 0.83625\n","Iteration 1300, lr =  1e-05 loss: 1.822167 acc: 0.83625\n","Iteration 1310, lr =  1e-05 loss: 1.8182946 acc: 0.8375\n","Iteration 1320, lr =  1e-05 loss: 1.8146857 acc: 0.83875\n","Iteration 1330, lr =  1e-05 loss: 1.8147614 acc: 0.836875\n","Iteration 1340, lr =  1e-05 loss: 1.8116546 acc: 0.836875\n","Iteration 1350, lr =  1e-05 loss: 1.8118628 acc: 0.83625\n","Iteration 1360, lr =  1e-05 loss: 1.8030362 acc: 0.84\n","Iteration 1370, lr =  1e-05 loss: 1.8008049 acc: 0.84125\n","Iteration 1380, lr =  1e-05 loss: 1.7992871 acc: 0.84125\n","Iteration 1390, lr =  1e-05 loss: 1.794372 acc: 0.844375\n","Iteration 1400, lr =  1e-05 loss: 1.7908646 acc: 0.844375\n","Iteration 1410, lr =  1e-05 loss: 1.7914516 acc: 0.8425\n","Iteration 1420, lr =  1e-05 loss: 1.791394 acc: 0.84125\n","Iteration 1430, lr =  1e-05 loss: 1.7895825 acc: 0.840625\n","Iteration 1440, lr =  1e-05 loss: 1.7884904 acc: 0.839375\n","Iteration 1450, lr =  1e-05 loss: 1.7892964 acc: 0.83875\n","Iteration 1460, lr =  1e-05 loss: 1.7870281 acc: 0.838125\n","Iteration 1470, lr =  1e-05 loss: 1.7856898 acc: 0.8375\n","Iteration 1480, lr =  1e-05 loss: 1.7879283 acc: 0.834375\n","Iteration 1490, lr =  1e-05 loss: 1.782773 acc: 0.83625\n","Iteration 1500, lr =  1e-05 loss: 1.7793856 acc: 0.836875\n","Iteration 1510, lr =  1e-05 loss: 1.7786171 acc: 0.83625\n","Iteration 1520, lr =  1e-05 loss: 1.7748959 acc: 0.836875\n","Iteration 1530, lr =  1e-05 loss: 1.7750636 acc: 0.83625\n","Iteration 1540, lr =  1e-05 loss: 1.7744201 acc: 0.835625\n","Iteration 1550, lr =  1e-05 loss: 1.7764996 acc: 0.834375\n","Iteration 1560, lr =  1e-05 loss: 1.7771122 acc: 0.834375\n","Iteration 1570, lr =  1e-05 loss: 1.7735187 acc: 0.83625\n","Iteration 1580, lr =  1e-05 loss: 1.7705516 acc: 0.838125\n","Iteration 1590, lr =  1e-05 loss: 1.7662486 acc: 0.83875\n","Iteration 1600, lr =  1e-05 loss: 1.7607994 acc: 0.84\n","Iteration 1610, lr =  1e-05 loss: 1.7607143 acc: 0.83875\n","Iteration 1620, lr =  1e-05 loss: 1.7584794 acc: 0.83875\n","Iteration 1630, lr =  1e-05 loss: 1.7603151 acc: 0.8375\n","Iteration 1640, lr =  1e-05 loss: 1.7573472 acc: 0.8375\n","Iteration 1650, lr =  1e-05 loss: 1.7544807 acc: 0.8375\n","Iteration 1660, lr =  1e-05 loss: 1.7546788 acc: 0.836875\n","Iteration 1670, lr =  1e-05 loss: 1.7538816 acc: 0.836875\n","Iteration 1680, lr =  1e-05 loss: 1.7521733 acc: 0.83625\n","Iteration 1690, lr =  1e-05 loss: 1.7449497 acc: 0.84\n","Iteration 1700, lr =  1e-05 loss: 1.7419634 acc: 0.840625\n","Iteration 1710, lr =  1e-05 loss: 1.7378569 acc: 0.841875\n","Iteration 1720, lr =  1e-05 loss: 1.7348938 acc: 0.841875\n","Iteration 1730, lr =  1e-05 loss: 1.7318519 acc: 0.843125\n","Iteration 1740, lr =  1e-05 loss: 1.7350781 acc: 0.84125\n","Iteration 1750, lr =  1e-05 loss: 1.732903 acc: 0.841875\n","Iteration 1760, lr =  1e-05 loss: 1.7305793 acc: 0.841875\n","Iteration 1770, lr =  1e-05 loss: 1.7280786 acc: 0.843125\n","Iteration 1780, lr =  1e-05 loss: 1.7290231 acc: 0.843125\n","Iteration 1790, lr =  1e-05 loss: 1.73032 acc: 0.841875\n","Iteration 1800, lr =  1e-05 loss: 1.7300768 acc: 0.84\n","Iteration 1810, lr =  1e-05 loss: 1.7322786 acc: 0.83875\n","Iteration 1820, lr =  1e-05 loss: 1.7319025 acc: 0.8375\n","Iteration 1830, lr =  1e-05 loss: 1.7310972 acc: 0.83625\n","Iteration 1840, lr =  1e-05 loss: 1.7298504 acc: 0.835625\n","Iteration 1850, lr =  1e-05 loss: 1.7270083 acc: 0.83625\n","Iteration 1860, lr =  1e-05 loss: 1.7248464 acc: 0.83625\n","Iteration 1870, lr =  1e-05 loss: 1.7191058 acc: 0.838125\n","Iteration 1880, lr =  1e-05 loss: 1.718564 acc: 0.838125\n","Iteration 1890, lr =  1e-05 loss: 1.7167782 acc: 0.83875\n","Iteration 1900, lr =  1e-05 loss: 1.7154177 acc: 0.838125\n","Iteration 1910, lr =  1e-05 loss: 1.7167008 acc: 0.838125\n","Iteration 1920, lr =  1e-05 loss: 1.7112018 acc: 0.84125\n","Iteration 1930, lr =  1e-05 loss: 1.7091193 acc: 0.840625\n","Iteration 1940, lr =  1e-05 loss: 1.7045398 acc: 0.84375\n","Iteration 1950, lr =  1e-05 loss: 1.7010814 acc: 0.845\n","Iteration 1960, lr =  1e-05 loss: 1.6956677 acc: 0.846875\n","Iteration 1970, lr =  1e-05 loss: 1.6915938 acc: 0.8475\n","Iteration 1980, lr =  1e-05 loss: 1.6878585 acc: 0.84875\n","Iteration 1990, lr =  1e-05 loss: 1.6801926 acc: 0.85125\n","Saving: ./drive/My Drive/project/result/gel-128-v5/training/net.tf\n","Iteration 2000, lr =  5e-06 loss: 1.6774554 acc: 0.85125\n","Iteration 2010, lr =  5e-06 loss: 1.6786284 acc: 0.85\n","Iteration 2020, lr =  5e-06 loss: 1.678974 acc: 0.848125\n","Iteration 2030, lr =  5e-06 loss: 1.6777437 acc: 0.8475\n","Iteration 2040, lr =  5e-06 loss: 1.6776983 acc: 0.8475\n","Iteration 2050, lr =  5e-06 loss: 1.6791703 acc: 0.845\n","Iteration 2060, lr =  5e-06 loss: 1.6747 acc: 0.8475\n","Iteration 2070, lr =  5e-06 loss: 1.6771375 acc: 0.845625\n","Iteration 2080, lr =  5e-06 loss: 1.6764364 acc: 0.845625\n","Iteration 2090, lr =  5e-06 loss: 1.6758641 acc: 0.84625\n","Iteration 2100, lr =  5e-06 loss: 1.675626 acc: 0.845625\n","Iteration 2110, lr =  5e-06 loss: 1.6780835 acc: 0.844375\n","Iteration 2120, lr =  5e-06 loss: 1.6777762 acc: 0.84375\n","Iteration 2130, lr =  5e-06 loss: 1.6772672 acc: 0.84375\n","Iteration 2140, lr =  5e-06 loss: 1.6780276 acc: 0.843125\n","Iteration 2150, lr =  5e-06 loss: 1.6751239 acc: 0.84625\n","Iteration 2160, lr =  5e-06 loss: 1.6733252 acc: 0.846875\n","Iteration 2170, lr =  5e-06 loss: 1.6727569 acc: 0.846875\n","Iteration 2180, lr =  5e-06 loss: 1.6659665 acc: 0.84875\n","Iteration 2190, lr =  5e-06 loss: 1.6649584 acc: 0.849375\n","Iteration 2200, lr =  5e-06 loss: 1.6636484 acc: 0.849375\n","Iteration 2210, lr =  5e-06 loss: 1.6624395 acc: 0.85\n","Iteration 2220, lr =  5e-06 loss: 1.6608526 acc: 0.850625\n","Iteration 2230, lr =  5e-06 loss: 1.6607493 acc: 0.85\n","Iteration 2240, lr =  5e-06 loss: 1.6573937 acc: 0.85125\n","Iteration 2250, lr =  5e-06 loss: 1.6551414 acc: 0.851875\n","Iteration 2260, lr =  5e-06 loss: 1.6571051 acc: 0.850625\n","Iteration 2270, lr =  5e-06 loss: 1.658508 acc: 0.850625\n","Iteration 2280, lr =  5e-06 loss: 1.657728 acc: 0.849375\n","Iteration 2290, lr =  5e-06 loss: 1.6567825 acc: 0.849375\n","Iteration 2300, lr =  5e-06 loss: 1.6552646 acc: 0.85\n","Iteration 2310, lr =  5e-06 loss: 1.6552707 acc: 0.84875\n","Iteration 2320, lr =  5e-06 loss: 1.655313 acc: 0.8475\n","Iteration 2330, lr =  5e-06 loss: 1.6549523 acc: 0.848125\n","Iteration 2340, lr =  5e-06 loss: 1.6542984 acc: 0.848125\n","Iteration 2350, lr =  5e-06 loss: 1.6494831 acc: 0.849375\n","Iteration 2360, lr =  5e-06 loss: 1.6468368 acc: 0.850625\n","Iteration 2370, lr =  5e-06 loss: 1.6426839 acc: 0.85125\n","Iteration 2380, lr =  5e-06 loss: 1.6393796 acc: 0.8525\n","Iteration 2390, lr =  5e-06 loss: 1.6357042 acc: 0.855\n","Iteration 2400, lr =  5e-06 loss: 1.6353781 acc: 0.854375\n","Iteration 2410, lr =  5e-06 loss: 1.6327785 acc: 0.855625\n","Iteration 2420, lr =  5e-06 loss: 1.6305106 acc: 0.85625\n","Iteration 2430, lr =  5e-06 loss: 1.6294609 acc: 0.856875\n","Iteration 2440, lr =  5e-06 loss: 1.6253488 acc: 0.85875\n","Iteration 2450, lr =  5e-06 loss: 1.6205475 acc: 0.86\n","Iteration 2460, lr =  5e-06 loss: 1.6168106 acc: 0.861875\n","Iteration 2470, lr =  5e-06 loss: 1.6144904 acc: 0.863125\n","Iteration 2480, lr =  5e-06 loss: 1.6058823 acc: 0.8675\n","Iteration 2490, lr =  5e-06 loss: 1.6043705 acc: 0.866875\n","Iteration 2500, lr =  5e-06 loss: 1.601577 acc: 0.86875\n","Iteration 2510, lr =  5e-06 loss: 1.5980964 acc: 0.87\n","Iteration 2520, lr =  5e-06 loss: 1.5977553 acc: 0.870625\n","Iteration 2530, lr =  5e-06 loss: 1.5959483 acc: 0.87125\n","Iteration 2540, lr =  5e-06 loss: 1.591829 acc: 0.873125\n","Iteration 2550, lr =  5e-06 loss: 1.585895 acc: 0.875625\n","Iteration 2560, lr =  5e-06 loss: 1.5832638 acc: 0.875625\n","Iteration 2570, lr =  5e-06 loss: 1.5816472 acc: 0.875\n","Iteration 2580, lr =  5e-06 loss: 1.576005 acc: 0.876875\n","Iteration 2590, lr =  5e-06 loss: 1.5745668 acc: 0.8775\n","Iteration 2600, lr =  5e-06 loss: 1.5735369 acc: 0.8775\n","Iteration 2610, lr =  5e-06 loss: 1.5749098 acc: 0.8775\n","Iteration 2620, lr =  5e-06 loss: 1.5752057 acc: 0.876875\n","Iteration 2630, lr =  5e-06 loss: 1.5693201 acc: 0.878125\n","Iteration 2640, lr =  5e-06 loss: 1.5695641 acc: 0.8775\n","Iteration 2650, lr =  5e-06 loss: 1.5719873 acc: 0.875\n","Iteration 2660, lr =  5e-06 loss: 1.5697616 acc: 0.875\n","Iteration 2670, lr =  5e-06 loss: 1.5684727 acc: 0.875\n","Iteration 2680, lr =  5e-06 loss: 1.5678526 acc: 0.875625\n","Iteration 2690, lr =  5e-06 loss: 1.5682853 acc: 0.875\n","Iteration 2700, lr =  5e-06 loss: 1.5679882 acc: 0.874375\n","Iteration 2710, lr =  5e-06 loss: 1.5685478 acc: 0.874375\n","Iteration 2720, lr =  5e-06 loss: 1.5682518 acc: 0.875\n","Iteration 2730, lr =  5e-06 loss: 1.5669084 acc: 0.875625\n","Iteration 2740, lr =  5e-06 loss: 1.561508 acc: 0.8775\n","Iteration 2750, lr =  5e-06 loss: 1.5618541 acc: 0.87625\n","Iteration 2760, lr =  5e-06 loss: 1.5634755 acc: 0.87625\n","Iteration 2770, lr =  5e-06 loss: 1.5646873 acc: 0.875625\n","Iteration 2780, lr =  5e-06 loss: 1.5630331 acc: 0.874375\n","Iteration 2790, lr =  5e-06 loss: 1.5607939 acc: 0.875625\n","Iteration 2800, lr =  5e-06 loss: 1.5613197 acc: 0.875625\n","Iteration 2810, lr =  5e-06 loss: 1.5575644 acc: 0.87625\n","Iteration 2820, lr =  5e-06 loss: 1.5562863 acc: 0.876875\n","Iteration 2830, lr =  5e-06 loss: 1.5541462 acc: 0.87875\n","Iteration 2840, lr =  5e-06 loss: 1.5550089 acc: 0.87875\n","Iteration 2850, lr =  5e-06 loss: 1.5526567 acc: 0.87875\n","Iteration 2860, lr =  5e-06 loss: 1.5508952 acc: 0.879375\n","Iteration 2870, lr =  5e-06 loss: 1.549746 acc: 0.879375\n","Iteration 2880, lr =  5e-06 loss: 1.5489175 acc: 0.879375\n","Iteration 2890, lr =  5e-06 loss: 1.5442963 acc: 0.881875\n","Iteration 2900, lr =  5e-06 loss: 1.5407501 acc: 0.88375\n","Iteration 2910, lr =  5e-06 loss: 1.5353343 acc: 0.885625\n","Iteration 2920, lr =  5e-06 loss: 1.5339563 acc: 0.885625\n","Iteration 2930, lr =  5e-06 loss: 1.5368503 acc: 0.88375\n","Iteration 2940, lr =  5e-06 loss: 1.5369399 acc: 0.883125\n","Iteration 2950, lr =  5e-06 loss: 1.5345813 acc: 0.884375\n","Iteration 2960, lr =  5e-06 loss: 1.5366582 acc: 0.8825\n","Iteration 2970, lr =  5e-06 loss: 1.5375422 acc: 0.881875\n","Iteration 2980, lr =  5e-06 loss: 1.5380789 acc: 0.88125\n","Iteration 2990, lr =  5e-06 loss: 1.5393639 acc: 0.880625\n","Saving: ./drive/My Drive/project/result/gel-128-v5/training/net.tf\n","Iteration 3000, lr =  5e-06 loss: 1.5370746 acc: 0.8825\n","Iteration 3010, lr =  5e-06 loss: 1.53247 acc: 0.884375\n","Iteration 3020, lr =  5e-06 loss: 1.5302025 acc: 0.885625\n","Iteration 3030, lr =  5e-06 loss: 1.5308921 acc: 0.884375\n","Iteration 3040, lr =  5e-06 loss: 1.5340552 acc: 0.8825\n","Iteration 3050, lr =  5e-06 loss: 1.5314085 acc: 0.884375\n","Iteration 3060, lr =  5e-06 loss: 1.5313077 acc: 0.884375\n","Iteration 3070, lr =  5e-06 loss: 1.5265427 acc: 0.88625\n","Iteration 3080, lr =  5e-06 loss: 1.5244064 acc: 0.886875\n","Iteration 3090, lr =  5e-06 loss: 1.5239687 acc: 0.88625\n","Iteration 3100, lr =  5e-06 loss: 1.5224398 acc: 0.88625\n","Iteration 3110, lr =  5e-06 loss: 1.5188315 acc: 0.886875\n","Iteration 3120, lr =  5e-06 loss: 1.5194957 acc: 0.888125\n","Iteration 3130, lr =  5e-06 loss: 1.5186101 acc: 0.888125\n","Iteration 3140, lr =  5e-06 loss: 1.5151665 acc: 0.889375\n","Iteration 3150, lr =  5e-06 loss: 1.5123541 acc: 0.89\n","Iteration 3160, lr =  5e-06 loss: 1.5122261 acc: 0.89\n","Iteration 3170, lr =  5e-06 loss: 1.5108472 acc: 0.89\n","Iteration 3180, lr =  5e-06 loss: 1.5101181 acc: 0.89\n","Iteration 3190, lr =  5e-06 loss: 1.5050546 acc: 0.893125\n","Iteration 3200, lr =  5e-06 loss: 1.5016396 acc: 0.895\n","Iteration 3210, lr =  5e-06 loss: 1.5014958 acc: 0.894375\n","Iteration 3220, lr =  5e-06 loss: 1.503056 acc: 0.893125\n","Iteration 3230, lr =  5e-06 loss: 1.5039704 acc: 0.8925\n","Iteration 3240, lr =  5e-06 loss: 1.505535 acc: 0.891875\n","Iteration 3250, lr =  5e-06 loss: 1.5052954 acc: 0.89125\n","Iteration 3260, lr =  5e-06 loss: 1.5035176 acc: 0.8925\n","Iteration 3270, lr =  5e-06 loss: 1.5035446 acc: 0.8925\n","Iteration 3280, lr =  5e-06 loss: 1.5026016 acc: 0.89375\n","Iteration 3290, lr =  5e-06 loss: 1.5024589 acc: 0.89375\n","Iteration 3300, lr =  5e-06 loss: 1.4993917 acc: 0.895\n","Iteration 3310, lr =  5e-06 loss: 1.499252 acc: 0.895625\n","Iteration 3320, lr =  5e-06 loss: 1.4996197 acc: 0.89625\n","Iteration 3330, lr =  5e-06 loss: 1.4984539 acc: 0.89625\n","Iteration 3340, lr =  5e-06 loss: 1.4972283 acc: 0.896875\n","Iteration 3350, lr =  5e-06 loss: 1.4990451 acc: 0.895625\n","Iteration 3360, lr =  5e-06 loss: 1.498847 acc: 0.895\n","Iteration 3370, lr =  5e-06 loss: 1.4996372 acc: 0.89375\n","Iteration 3380, lr =  5e-06 loss: 1.4997165 acc: 0.893125\n","Iteration 3390, lr =  5e-06 loss: 1.4997015 acc: 0.8925\n","Iteration 3400, lr =  5e-06 loss: 1.4982383 acc: 0.89375\n","Iteration 3410, lr =  5e-06 loss: 1.4979712 acc: 0.8925\n","Iteration 3420, lr =  5e-06 loss: 1.4974089 acc: 0.8925\n","Iteration 3430, lr =  5e-06 loss: 1.4975691 acc: 0.89125\n","Iteration 3440, lr =  5e-06 loss: 1.4987689 acc: 0.890625\n","Iteration 3450, lr =  5e-06 loss: 1.5003444 acc: 0.89\n","Iteration 3460, lr =  5e-06 loss: 1.5021913 acc: 0.889375\n","Iteration 3470, lr =  5e-06 loss: 1.5069557 acc: 0.886875\n","Iteration 3480, lr =  5e-06 loss: 1.5073401 acc: 0.88625\n","Iteration 3490, lr =  5e-06 loss: 1.5079902 acc: 0.88625\n","Iteration 3500, lr =  5e-06 loss: 1.5081944 acc: 0.885\n","Iteration 3510, lr =  5e-06 loss: 1.5074279 acc: 0.885625\n","Iteration 3520, lr =  5e-06 loss: 1.5061914 acc: 0.885625\n","Iteration 3530, lr =  5e-06 loss: 1.5080332 acc: 0.883125\n","Iteration 3540, lr =  5e-06 loss: 1.5091847 acc: 0.8825\n","Iteration 3550, lr =  5e-06 loss: 1.5097276 acc: 0.8825\n","Iteration 3560, lr =  5e-06 loss: 1.5077052 acc: 0.88375\n","Iteration 3570, lr =  5e-06 loss: 1.5094473 acc: 0.883125\n","Iteration 3580, lr =  5e-06 loss: 1.5121748 acc: 0.880625\n","Iteration 3590, lr =  5e-06 loss: 1.5109433 acc: 0.880625\n","Iteration 3600, lr =  5e-06 loss: 1.5091149 acc: 0.880625\n","Iteration 3610, lr =  5e-06 loss: 1.5041711 acc: 0.8825\n","Iteration 3620, lr =  5e-06 loss: 1.5021625 acc: 0.883125\n","Iteration 3630, lr =  5e-06 loss: 1.5042441 acc: 0.8825\n","Iteration 3640, lr =  5e-06 loss: 1.5034376 acc: 0.881875\n","Iteration 3650, lr =  5e-06 loss: 1.498526 acc: 0.885625\n","Iteration 3660, lr =  5e-06 loss: 1.4965601 acc: 0.88625\n","Iteration 3670, lr =  5e-06 loss: 1.4949745 acc: 0.886875\n","Iteration 3680, lr =  5e-06 loss: 1.4939322 acc: 0.8875\n","Iteration 3690, lr =  5e-06 loss: 1.4925691 acc: 0.886875\n","Iteration 3700, lr =  5e-06 loss: 1.4910746 acc: 0.888125\n","Iteration 3710, lr =  5e-06 loss: 1.4889233 acc: 0.88875\n","Iteration 3720, lr =  5e-06 loss: 1.4904351 acc: 0.88625\n","Iteration 3730, lr =  5e-06 loss: 1.4912086 acc: 0.885\n","Iteration 3740, lr =  5e-06 loss: 1.4900911 acc: 0.885625\n","Iteration 3750, lr =  5e-06 loss: 1.4900333 acc: 0.885\n","Iteration 3760, lr =  5e-06 loss: 1.4854463 acc: 0.886875\n","Iteration 3770, lr =  5e-06 loss: 1.4841357 acc: 0.88625\n","Iteration 3780, lr =  5e-06 loss: 1.4837334 acc: 0.8875\n","Iteration 3790, lr =  5e-06 loss: 1.4842764 acc: 0.886875\n","Iteration 3800, lr =  5e-06 loss: 1.4847549 acc: 0.88625\n","Iteration 3810, lr =  5e-06 loss: 1.4847149 acc: 0.885625\n","Iteration 3820, lr =  5e-06 loss: 1.4843303 acc: 0.88625\n","Iteration 3830, lr =  5e-06 loss: 1.4853166 acc: 0.885625\n","Iteration 3840, lr =  5e-06 loss: 1.4851558 acc: 0.884375\n","Iteration 3850, lr =  5e-06 loss: 1.4852562 acc: 0.885\n","Iteration 3860, lr =  5e-06 loss: 1.4863281 acc: 0.883125\n","Iteration 3870, lr =  5e-06 loss: 1.4864855 acc: 0.8825\n","Iteration 3880, lr =  5e-06 loss: 1.4852209 acc: 0.881875\n","Iteration 3890, lr =  5e-06 loss: 1.4847357 acc: 0.881875\n","Iteration 3900, lr =  5e-06 loss: 1.4843712 acc: 0.881875\n","Iteration 3910, lr =  5e-06 loss: 1.4828395 acc: 0.881875\n","Iteration 3920, lr =  5e-06 loss: 1.4829327 acc: 0.881875\n","Iteration 3930, lr =  5e-06 loss: 1.4807913 acc: 0.88375\n","Iteration 3940, lr =  5e-06 loss: 1.4798326 acc: 0.88375\n","Iteration 3950, lr =  5e-06 loss: 1.4814109 acc: 0.8825\n","Iteration 3960, lr =  5e-06 loss: 1.4801962 acc: 0.88375\n","Iteration 3970, lr =  5e-06 loss: 1.4797745 acc: 0.88375\n","Iteration 3980, lr =  5e-06 loss: 1.4818337 acc: 0.88375\n","Iteration 3990, lr =  5e-06 loss: 1.4770371 acc: 0.885625\n","Saving: ./drive/My Drive/project/result/gel-128-v5/training/net.tf\n","Iteration 4000, lr =  2.5e-06 loss: 1.4777362 acc: 0.88375\n","Iteration 4010, lr =  2.5e-06 loss: 1.4777801 acc: 0.8825\n","Iteration 4020, lr =  2.5e-06 loss: 1.4774272 acc: 0.881875\n","Iteration 4030, lr =  2.5e-06 loss: 1.4762702 acc: 0.88375\n","Iteration 4040, lr =  2.5e-06 loss: 1.4716008 acc: 0.885625\n","Iteration 4050, lr =  2.5e-06 loss: 1.4700441 acc: 0.886875\n","Iteration 4060, lr =  2.5e-06 loss: 1.4684294 acc: 0.886875\n","Iteration 4070, lr =  2.5e-06 loss: 1.4711205 acc: 0.885\n","Iteration 4080, lr =  2.5e-06 loss: 1.4695892 acc: 0.885625\n","Iteration 4090, lr =  2.5e-06 loss: 1.4695164 acc: 0.884375\n","Iteration 4100, lr =  2.5e-06 loss: 1.468532 acc: 0.885625\n","Iteration 4110, lr =  2.5e-06 loss: 1.4739506 acc: 0.884375\n","Iteration 4120, lr =  2.5e-06 loss: 1.4715272 acc: 0.88375\n","Iteration 4130, lr =  2.5e-06 loss: 1.4743352 acc: 0.881875\n","Iteration 4140, lr =  2.5e-06 loss: 1.4758775 acc: 0.880625\n","Iteration 4150, lr =  2.5e-06 loss: 1.4772179 acc: 0.88\n","Iteration 4160, lr =  2.5e-06 loss: 1.4782732 acc: 0.879375\n","Iteration 4170, lr =  2.5e-06 loss: 1.4776798 acc: 0.879375\n","Iteration 4180, lr =  2.5e-06 loss: 1.4784204 acc: 0.87875\n","Iteration 4190, lr =  2.5e-06 loss: 1.4769516 acc: 0.879375\n","Iteration 4200, lr =  2.5e-06 loss: 1.480048 acc: 0.8775\n","Iteration 4210, lr =  2.5e-06 loss: 1.4808239 acc: 0.875625\n","Iteration 4220, lr =  2.5e-06 loss: 1.4787626 acc: 0.876875\n","Iteration 4230, lr =  2.5e-06 loss: 1.4751097 acc: 0.878125\n","Iteration 4240, lr =  2.5e-06 loss: 1.4769627 acc: 0.876875\n","Iteration 4250, lr =  2.5e-06 loss: 1.4804735 acc: 0.875\n","Iteration 4260, lr =  2.5e-06 loss: 1.4816813 acc: 0.87375\n","Iteration 4270, lr =  2.5e-06 loss: 1.478764 acc: 0.87375\n","Iteration 4280, lr =  2.5e-06 loss: 1.4768453 acc: 0.873125\n","Iteration 4290, lr =  2.5e-06 loss: 1.4750333 acc: 0.87375\n","Iteration 4300, lr =  2.5e-06 loss: 1.4767157 acc: 0.87125\n","Iteration 4310, lr =  2.5e-06 loss: 1.4767853 acc: 0.87125\n","Iteration 4320, lr =  2.5e-06 loss: 1.4728612 acc: 0.8725\n","Iteration 4330, lr =  2.5e-06 loss: 1.4709319 acc: 0.873125\n","Iteration 4340, lr =  2.5e-06 loss: 1.469345 acc: 0.87375\n","Iteration 4350, lr =  2.5e-06 loss: 1.4684159 acc: 0.873125\n","Iteration 4360, lr =  2.5e-06 loss: 1.46829 acc: 0.873125\n","Iteration 4370, lr =  2.5e-06 loss: 1.4674964 acc: 0.873125\n","Iteration 4380, lr =  2.5e-06 loss: 1.4692618 acc: 0.870625\n","Iteration 4390, lr =  2.5e-06 loss: 1.4700332 acc: 0.87\n","Iteration 4400, lr =  2.5e-06 loss: 1.4707649 acc: 0.869375\n","Iteration 4410, lr =  2.5e-06 loss: 1.4754093 acc: 0.866875\n","Iteration 4420, lr =  2.5e-06 loss: 1.4750785 acc: 0.8675\n","Iteration 4430, lr =  2.5e-06 loss: 1.4731803 acc: 0.869375\n","Iteration 4440, lr =  2.5e-06 loss: 1.47027 acc: 0.87\n","Iteration 4450, lr =  2.5e-06 loss: 1.4675583 acc: 0.870625\n","Iteration 4460, lr =  2.5e-06 loss: 1.4665506 acc: 0.87\n","Iteration 4470, lr =  2.5e-06 loss: 1.4607636 acc: 0.871875\n","Iteration 4480, lr =  2.5e-06 loss: 1.4600874 acc: 0.8725\n","Iteration 4490, lr =  2.5e-06 loss: 1.457743 acc: 0.873125\n","Iteration 4500, lr =  2.5e-06 loss: 1.4562278 acc: 0.87375\n","Iteration 4510, lr =  2.5e-06 loss: 1.4540656 acc: 0.874375\n","Iteration 4520, lr =  2.5e-06 loss: 1.4531082 acc: 0.874375\n","Iteration 4530, lr =  2.5e-06 loss: 1.4483997 acc: 0.876875\n","Iteration 4540, lr =  2.5e-06 loss: 1.4468505 acc: 0.8775\n","Iteration 4550, lr =  2.5e-06 loss: 1.4451901 acc: 0.8775\n","Iteration 4560, lr =  2.5e-06 loss: 1.4447926 acc: 0.8775\n","Iteration 4570, lr =  2.5e-06 loss: 1.4436222 acc: 0.878125\n","Iteration 4580, lr =  2.5e-06 loss: 1.4414259 acc: 0.88\n","Iteration 4590, lr =  2.5e-06 loss: 1.4430742 acc: 0.879375\n","Iteration 4600, lr =  2.5e-06 loss: 1.4450914 acc: 0.87875\n","Iteration 4610, lr =  2.5e-06 loss: 1.4466761 acc: 0.876875\n","Iteration 4620, lr =  2.5e-06 loss: 1.4498339 acc: 0.875625\n","Iteration 4630, lr =  2.5e-06 loss: 1.4498787 acc: 0.875625\n","Iteration 4640, lr =  2.5e-06 loss: 1.4508933 acc: 0.875625\n","Iteration 4650, lr =  2.5e-06 loss: 1.4512124 acc: 0.875625\n","Iteration 4660, lr =  2.5e-06 loss: 1.4507707 acc: 0.875625\n","Iteration 4670, lr =  2.5e-06 loss: 1.4539132 acc: 0.875\n","Iteration 4680, lr =  2.5e-06 loss: 1.4559561 acc: 0.874375\n","Iteration 4690, lr =  2.5e-06 loss: 1.4577566 acc: 0.874375\n","Iteration 4700, lr =  2.5e-06 loss: 1.4597006 acc: 0.87375\n","Iteration 4710, lr =  2.5e-06 loss: 1.4612076 acc: 0.873125\n","Iteration 4720, lr =  2.5e-06 loss: 1.458858 acc: 0.875\n","Iteration 4730, lr =  2.5e-06 loss: 1.4582022 acc: 0.875\n","Iteration 4740, lr =  2.5e-06 loss: 1.457285 acc: 0.875\n","Iteration 4750, lr =  2.5e-06 loss: 1.4575546 acc: 0.87625\n","Iteration 4760, lr =  2.5e-06 loss: 1.4585369 acc: 0.875625\n","Iteration 4770, lr =  2.5e-06 loss: 1.4580504 acc: 0.875625\n","Iteration 4780, lr =  2.5e-06 loss: 1.458695 acc: 0.875\n","Iteration 4790, lr =  2.5e-06 loss: 1.4597632 acc: 0.873125\n","Iteration 4800, lr =  2.5e-06 loss: 1.4593191 acc: 0.87375\n","Iteration 4810, lr =  2.5e-06 loss: 1.4572393 acc: 0.875\n","Iteration 4820, lr =  2.5e-06 loss: 1.4560264 acc: 0.875\n","Iteration 4830, lr =  2.5e-06 loss: 1.4531665 acc: 0.875625\n","Iteration 4840, lr =  2.5e-06 loss: 1.4509414 acc: 0.8775\n","Iteration 4850, lr =  2.5e-06 loss: 1.4514709 acc: 0.8775\n","Iteration 4860, lr =  2.5e-06 loss: 1.4487636 acc: 0.88\n","Iteration 4870, lr =  2.5e-06 loss: 1.446545 acc: 0.881875\n","Iteration 4880, lr =  2.5e-06 loss: 1.4476875 acc: 0.881875\n","Iteration 4890, lr =  2.5e-06 loss: 1.4492848 acc: 0.88125\n","Iteration 4900, lr =  2.5e-06 loss: 1.4503975 acc: 0.88\n","Iteration 4910, lr =  2.5e-06 loss: 1.4515398 acc: 0.88\n","Iteration 4920, lr =  2.5e-06 loss: 1.4522941 acc: 0.879375\n","Iteration 4930, lr =  2.5e-06 loss: 1.4498216 acc: 0.880625\n","Iteration 4940, lr =  2.5e-06 loss: 1.453876 acc: 0.87875\n","Iteration 4950, lr =  2.5e-06 loss: 1.4535158 acc: 0.88\n","Iteration 4960, lr =  2.5e-06 loss: 1.4544023 acc: 0.88\n","Iteration 4970, lr =  2.5e-06 loss: 1.4532824 acc: 0.880625\n","Iteration 4980, lr =  2.5e-06 loss: 1.4507797 acc: 0.88125\n","Iteration 4990, lr =  2.5e-06 loss: 1.4526309 acc: 0.88\n","Saving: ./drive/My Drive/project/result/gel-128-v5/training/net.tf\n","Iteration 5000, lr =  2.5e-06 loss: 1.4536356 acc: 0.88\n","Iteration 5010, lr =  2.5e-06 loss: 1.4539905 acc: 0.880625\n","Iteration 5020, lr =  2.5e-06 loss: 1.4580319 acc: 0.879375\n","Iteration 5030, lr =  2.5e-06 loss: 1.4555764 acc: 0.88\n","Iteration 5040, lr =  2.5e-06 loss: 1.4556036 acc: 0.879375\n","Iteration 5050, lr =  2.5e-06 loss: 1.4571666 acc: 0.878125\n","Iteration 5060, lr =  2.5e-06 loss: 1.4618313 acc: 0.876875\n","Iteration 5070, lr =  2.5e-06 loss: 1.4604052 acc: 0.8775\n","Iteration 5080, lr =  2.5e-06 loss: 1.4620335 acc: 0.876875\n","Iteration 5090, lr =  2.5e-06 loss: 1.4612548 acc: 0.878125\n","Iteration 5100, lr =  2.5e-06 loss: 1.4633925 acc: 0.87625\n","Iteration 5110, lr =  2.5e-06 loss: 1.4587787 acc: 0.878125\n","Iteration 5120, lr =  2.5e-06 loss: 1.4575039 acc: 0.87875\n","Iteration 5130, lr =  2.5e-06 loss: 1.4555387 acc: 0.879375\n","Iteration 5140, lr =  2.5e-06 loss: 1.452394 acc: 0.88125\n","Iteration 5150, lr =  2.5e-06 loss: 1.449089 acc: 0.883125\n","Iteration 5160, lr =  2.5e-06 loss: 1.4486469 acc: 0.8825\n","Iteration 5170, lr =  2.5e-06 loss: 1.448502 acc: 0.883125\n","Iteration 5180, lr =  2.5e-06 loss: 1.4492763 acc: 0.883125\n","Iteration 5190, lr =  2.5e-06 loss: 1.4497958 acc: 0.8825\n","Iteration 5200, lr =  2.5e-06 loss: 1.4479228 acc: 0.881875\n","Iteration 5210, lr =  2.5e-06 loss: 1.4466802 acc: 0.883125\n","Iteration 5220, lr =  2.5e-06 loss: 1.4474232 acc: 0.881875\n","Iteration 5230, lr =  2.5e-06 loss: 1.4482026 acc: 0.881875\n","Iteration 5240, lr =  2.5e-06 loss: 1.444744 acc: 0.88375\n","Iteration 5250, lr =  2.5e-06 loss: 1.4406402 acc: 0.88625\n","Iteration 5260, lr =  2.5e-06 loss: 1.4380547 acc: 0.8875\n","Iteration 5270, lr =  2.5e-06 loss: 1.4363835 acc: 0.88875\n","Iteration 5280, lr =  2.5e-06 loss: 1.4349072 acc: 0.89\n","Iteration 5290, lr =  2.5e-06 loss: 1.4356208 acc: 0.889375\n","Iteration 5300, lr =  2.5e-06 loss: 1.434872 acc: 0.89125\n","Iteration 5310, lr =  2.5e-06 loss: 1.4339242 acc: 0.890625\n","Iteration 5320, lr =  2.5e-06 loss: 1.4357282 acc: 0.888125\n","Iteration 5330, lr =  2.5e-06 loss: 1.4368991 acc: 0.886875\n","Iteration 5340, lr =  2.5e-06 loss: 1.4375548 acc: 0.88625\n","Iteration 5350, lr =  2.5e-06 loss: 1.4356873 acc: 0.8875\n","Iteration 5360, lr =  2.5e-06 loss: 1.4345486 acc: 0.888125\n","Iteration 5370, lr =  2.5e-06 loss: 1.4334149 acc: 0.889375\n","Iteration 5380, lr =  2.5e-06 loss: 1.4296991 acc: 0.893125\n","Iteration 5390, lr =  2.5e-06 loss: 1.429826 acc: 0.893125\n","Iteration 5400, lr =  2.5e-06 loss: 1.4289987 acc: 0.89375\n","Iteration 5410, lr =  2.5e-06 loss: 1.42219 acc: 0.89875\n","Iteration 5420, lr =  2.5e-06 loss: 1.4213333 acc: 0.89875\n","Iteration 5430, lr =  2.5e-06 loss: 1.4205381 acc: 0.898125\n","Iteration 5440, lr =  2.5e-06 loss: 1.4202015 acc: 0.89875\n","Iteration 5450, lr =  2.5e-06 loss: 1.4191324 acc: 0.899375\n","Iteration 5460, lr =  2.5e-06 loss: 1.4186325 acc: 0.9\n","Iteration 5470, lr =  2.5e-06 loss: 1.4207228 acc: 0.899375\n","Iteration 5480, lr =  2.5e-06 loss: 1.4204476 acc: 0.89875\n","Iteration 5490, lr =  2.5e-06 loss: 1.4212891 acc: 0.89875\n","Iteration 5500, lr =  2.5e-06 loss: 1.4221424 acc: 0.898125\n","Iteration 5510, lr =  2.5e-06 loss: 1.4229534 acc: 0.8975\n","Iteration 5520, lr =  2.5e-06 loss: 1.4250968 acc: 0.896875\n","Iteration 5530, lr =  2.5e-06 loss: 1.4252014 acc: 0.896875\n","Iteration 5540, lr =  2.5e-06 loss: 1.4286664 acc: 0.895\n","Iteration 5550, lr =  2.5e-06 loss: 1.4299572 acc: 0.894375\n","Iteration 5560, lr =  2.5e-06 loss: 1.4324275 acc: 0.893125\n","Iteration 5570, lr =  2.5e-06 loss: 1.4334073 acc: 0.8925\n","Iteration 5580, lr =  2.5e-06 loss: 1.4361955 acc: 0.89125\n","Iteration 5590, lr =  2.5e-06 loss: 1.4348669 acc: 0.891875\n","Iteration 5600, lr =  2.5e-06 loss: 1.4357896 acc: 0.89\n","Iteration 5610, lr =  2.5e-06 loss: 1.4400128 acc: 0.889375\n","Iteration 5620, lr =  2.5e-06 loss: 1.4393448 acc: 0.88875\n","Iteration 5630, lr =  2.5e-06 loss: 1.4358621 acc: 0.889375\n","Iteration 5640, lr =  2.5e-06 loss: 1.4320996 acc: 0.891875\n","Iteration 5650, lr =  2.5e-06 loss: 1.4315338 acc: 0.891875\n","Iteration 5660, lr =  2.5e-06 loss: 1.4308887 acc: 0.891875\n","Iteration 5670, lr =  2.5e-06 loss: 1.4260255 acc: 0.89375\n","Iteration 5680, lr =  2.5e-06 loss: 1.4248695 acc: 0.89375\n","Iteration 5690, lr =  2.5e-06 loss: 1.42629 acc: 0.893125\n","Iteration 5700, lr =  2.5e-06 loss: 1.4255376 acc: 0.893125\n","Iteration 5710, lr =  2.5e-06 loss: 1.4247326 acc: 0.8925\n","Iteration 5720, lr =  2.5e-06 loss: 1.4233955 acc: 0.89375\n","Iteration 5730, lr =  2.5e-06 loss: 1.4242387 acc: 0.894375\n","Iteration 5740, lr =  2.5e-06 loss: 1.4267197 acc: 0.89375\n","Iteration 5750, lr =  2.5e-06 loss: 1.4251449 acc: 0.89375\n","Iteration 5760, lr =  2.5e-06 loss: 1.4244342 acc: 0.89375\n","Iteration 5770, lr =  2.5e-06 loss: 1.4246536 acc: 0.894375\n","Iteration 5780, lr =  2.5e-06 loss: 1.4219965 acc: 0.895625\n","Iteration 5790, lr =  2.5e-06 loss: 1.4188043 acc: 0.898125\n","Iteration 5800, lr =  2.5e-06 loss: 1.4151706 acc: 0.9\n","Iteration 5810, lr =  2.5e-06 loss: 1.415545 acc: 0.9\n","Iteration 5820, lr =  2.5e-06 loss: 1.4170749 acc: 0.899375\n","Iteration 5830, lr =  2.5e-06 loss: 1.4181228 acc: 0.89875\n","Iteration 5840, lr =  2.5e-06 loss: 1.4176828 acc: 0.898125\n","Iteration 5850, lr =  2.5e-06 loss: 1.4157768 acc: 0.898125\n","Iteration 5860, lr =  2.5e-06 loss: 1.4169135 acc: 0.89875\n","Iteration 5870, lr =  2.5e-06 loss: 1.4190209 acc: 0.898125\n","Iteration 5880, lr =  2.5e-06 loss: 1.415906 acc: 0.899375\n","Iteration 5890, lr =  2.5e-06 loss: 1.4153656 acc: 0.899375\n","Iteration 5900, lr =  2.5e-06 loss: 1.413601 acc: 0.900625\n","Iteration 5910, lr =  2.5e-06 loss: 1.4107953 acc: 0.901875\n","Iteration 5920, lr =  2.5e-06 loss: 1.4091297 acc: 0.903125\n","Iteration 5930, lr =  2.5e-06 loss: 1.4065299 acc: 0.90375\n","Iteration 5940, lr =  2.5e-06 loss: 1.4012433 acc: 0.90625\n","Iteration 5950, lr =  2.5e-06 loss: 1.3990352 acc: 0.905625\n","Iteration 5960, lr =  2.5e-06 loss: 1.3973742 acc: 0.905\n","Iteration 5970, lr =  2.5e-06 loss: 1.3966322 acc: 0.905\n","Iteration 5980, lr =  2.5e-06 loss: 1.3944098 acc: 0.90625\n","Iteration 5990, lr =  2.5e-06 loss: 1.39196 acc: 0.9075\n","Saving: ./drive/My Drive/project/result/gel-128-v5/training/net.tf\n","Iteration 6000, lr =  1.25e-06 loss: 1.3887464 acc: 0.909375\n","Iteration 6010, lr =  1.25e-06 loss: 1.3882562 acc: 0.908125\n","Iteration 6020, lr =  1.25e-06 loss: 1.3844234 acc: 0.91\n","Iteration 6030, lr =  1.25e-06 loss: 1.3833412 acc: 0.91\n","Iteration 6040, lr =  1.25e-06 loss: 1.3842287 acc: 0.909375\n","Iteration 6050, lr =  1.25e-06 loss: 1.3828878 acc: 0.910625\n","Iteration 6060, lr =  1.25e-06 loss: 1.3774642 acc: 0.9125\n","Iteration 6070, lr =  1.25e-06 loss: 1.3757827 acc: 0.913125\n","Iteration 6080, lr =  1.25e-06 loss: 1.3756756 acc: 0.913125\n","Iteration 6090, lr =  1.25e-06 loss: 1.3756058 acc: 0.9125\n","Iteration 6100, lr =  1.25e-06 loss: 1.3762378 acc: 0.9125\n","Iteration 6110, lr =  1.25e-06 loss: 1.3757459 acc: 0.9125\n","Iteration 6120, lr =  1.25e-06 loss: 1.3790259 acc: 0.91125\n","Iteration 6130, lr =  1.25e-06 loss: 1.3801017 acc: 0.91125\n","Iteration 6140, lr =  1.25e-06 loss: 1.381715 acc: 0.91\n","Iteration 6150, lr =  1.25e-06 loss: 1.3823894 acc: 0.90875\n","Iteration 6160, lr =  1.25e-06 loss: 1.3812103 acc: 0.909375\n","Iteration 6170, lr =  1.25e-06 loss: 1.380346 acc: 0.909375\n","Iteration 6180, lr =  1.25e-06 loss: 1.3779619 acc: 0.91\n","Iteration 6190, lr =  1.25e-06 loss: 1.3774517 acc: 0.91\n","Iteration 6200, lr =  1.25e-06 loss: 1.3764943 acc: 0.911875\n","Iteration 6210, lr =  1.25e-06 loss: 1.3774997 acc: 0.911875\n","Iteration 6220, lr =  1.25e-06 loss: 1.3756856 acc: 0.913125\n","Iteration 6230, lr =  1.25e-06 loss: 1.3762709 acc: 0.9125\n","Iteration 6240, lr =  1.25e-06 loss: 1.378359 acc: 0.911875\n","Iteration 6250, lr =  1.25e-06 loss: 1.3780862 acc: 0.911875\n","Iteration 6260, lr =  1.25e-06 loss: 1.3764645 acc: 0.9125\n","Iteration 6270, lr =  1.25e-06 loss: 1.3761258 acc: 0.9125\n","Iteration 6280, lr =  1.25e-06 loss: 1.3764215 acc: 0.911875\n","Iteration 6290, lr =  1.25e-06 loss: 1.3765879 acc: 0.911875\n","Iteration 6300, lr =  1.25e-06 loss: 1.3756135 acc: 0.9125\n","Iteration 6310, lr =  1.25e-06 loss: 1.3754004 acc: 0.9125\n","Iteration 6320, lr =  1.25e-06 loss: 1.3753554 acc: 0.91375\n","Iteration 6330, lr =  1.25e-06 loss: 1.3733388 acc: 0.915625\n","Iteration 6340, lr =  1.25e-06 loss: 1.3733644 acc: 0.915\n","Iteration 6350, lr =  1.25e-06 loss: 1.3723128 acc: 0.915\n","Iteration 6360, lr =  1.25e-06 loss: 1.3757524 acc: 0.9125\n","Iteration 6370, lr =  1.25e-06 loss: 1.3792278 acc: 0.91125\n","Iteration 6380, lr =  1.25e-06 loss: 1.378417 acc: 0.91125\n","Iteration 6390, lr =  1.25e-06 loss: 1.3767718 acc: 0.91125\n","Iteration 6400, lr =  1.25e-06 loss: 1.3753837 acc: 0.911875\n","Iteration 6410, lr =  1.25e-06 loss: 1.3759137 acc: 0.91125\n","Iteration 6420, lr =  1.25e-06 loss: 1.3756037 acc: 0.91125\n","Iteration 6430, lr =  1.25e-06 loss: 1.375344 acc: 0.91125\n","Iteration 6440, lr =  1.25e-06 loss: 1.3756583 acc: 0.910625\n","Iteration 6450, lr =  1.25e-06 loss: 1.3760148 acc: 0.91\n","Iteration 6460, lr =  1.25e-06 loss: 1.3743005 acc: 0.910625\n","Iteration 6470, lr =  1.25e-06 loss: 1.3709253 acc: 0.911875\n","Iteration 6480, lr =  1.25e-06 loss: 1.371465 acc: 0.9125\n","Iteration 6490, lr =  1.25e-06 loss: 1.3700261 acc: 0.913125\n","Iteration 6500, lr =  1.25e-06 loss: 1.368768 acc: 0.91375\n","Iteration 6510, lr =  1.25e-06 loss: 1.3696587 acc: 0.91375\n","Iteration 6520, lr =  1.25e-06 loss: 1.3687512 acc: 0.914375\n","Iteration 6530, lr =  1.25e-06 loss: 1.3702152 acc: 0.91375\n","Iteration 6540, lr =  1.25e-06 loss: 1.3665862 acc: 0.915\n","Iteration 6550, lr =  1.25e-06 loss: 1.3683604 acc: 0.915625\n","Iteration 6560, lr =  1.25e-06 loss: 1.3667449 acc: 0.915625\n","Iteration 6570, lr =  1.25e-06 loss: 1.364934 acc: 0.916875\n","Iteration 6580, lr =  1.25e-06 loss: 1.3620291 acc: 0.918125\n","Iteration 6590, lr =  1.25e-06 loss: 1.3616818 acc: 0.91875\n","Iteration 6600, lr =  1.25e-06 loss: 1.3604007 acc: 0.920625\n","Iteration 6610, lr =  1.25e-06 loss: 1.3549756 acc: 0.9225\n","Iteration 6620, lr =  1.25e-06 loss: 1.3525527 acc: 0.924375\n","Iteration 6630, lr =  1.25e-06 loss: 1.3542234 acc: 0.92375\n","Iteration 6640, lr =  1.25e-06 loss: 1.3568869 acc: 0.9225\n","Iteration 6650, lr =  1.25e-06 loss: 1.3576812 acc: 0.92\n","Iteration 6660, lr =  1.25e-06 loss: 1.358083 acc: 0.92\n","Iteration 6670, lr =  1.25e-06 loss: 1.3600044 acc: 0.91875\n","Iteration 6680, lr =  1.25e-06 loss: 1.3584067 acc: 0.919375\n","Iteration 6690, lr =  1.25e-06 loss: 1.3537169 acc: 0.92125\n","Iteration 6700, lr =  1.25e-06 loss: 1.3527287 acc: 0.92125\n","Iteration 6710, lr =  1.25e-06 loss: 1.3524646 acc: 0.921875\n","Iteration 6720, lr =  1.25e-06 loss: 1.3527178 acc: 0.921875\n","Iteration 6730, lr =  1.25e-06 loss: 1.3542022 acc: 0.92\n","Iteration 6740, lr =  1.25e-06 loss: 1.3540313 acc: 0.919375\n","Iteration 6750, lr =  1.25e-06 loss: 1.3547484 acc: 0.91875\n","Iteration 6760, lr =  1.25e-06 loss: 1.3569618 acc: 0.916875\n","Iteration 6770, lr =  1.25e-06 loss: 1.3550944 acc: 0.9175\n","Iteration 6780, lr =  1.25e-06 loss: 1.3546323 acc: 0.9175\n","Iteration 6790, lr =  1.25e-06 loss: 1.3561145 acc: 0.915625\n","Iteration 6800, lr =  1.25e-06 loss: 1.359036 acc: 0.913125\n","Iteration 6810, lr =  1.25e-06 loss: 1.3606148 acc: 0.911875\n","Iteration 6820, lr =  1.25e-06 loss: 1.3582829 acc: 0.913125\n","Iteration 6830, lr =  1.25e-06 loss: 1.3570619 acc: 0.91375\n","Iteration 6840, lr =  1.25e-06 loss: 1.358908 acc: 0.9125\n","Iteration 6850, lr =  1.25e-06 loss: 1.3590498 acc: 0.9125\n","Iteration 6860, lr =  1.25e-06 loss: 1.3584405 acc: 0.91125\n","Iteration 6870, lr =  1.25e-06 loss: 1.3588398 acc: 0.91125\n","Iteration 6880, lr =  1.25e-06 loss: 1.3610222 acc: 0.91\n","Iteration 6890, lr =  1.25e-06 loss: 1.358849 acc: 0.910625\n","Iteration 6900, lr =  1.25e-06 loss: 1.3588239 acc: 0.910625\n","Iteration 6910, lr =  1.25e-06 loss: 1.3585205 acc: 0.910625\n","Iteration 6920, lr =  1.25e-06 loss: 1.3577996 acc: 0.910625\n","Iteration 6930, lr =  1.25e-06 loss: 1.3582588 acc: 0.91\n","Iteration 6940, lr =  1.25e-06 loss: 1.3596884 acc: 0.90875\n","Iteration 6950, lr =  1.25e-06 loss: 1.3595375 acc: 0.909375\n","Iteration 6960, lr =  1.25e-06 loss: 1.3577744 acc: 0.91125\n","Iteration 6970, lr =  1.25e-06 loss: 1.3583316 acc: 0.91125\n","Iteration 6980, lr =  1.25e-06 loss: 1.3575574 acc: 0.91125\n","Iteration 6990, lr =  1.25e-06 loss: 1.3599793 acc: 0.909375\n","Saving: ./drive/My Drive/project/result/gel-128-v5/training/net.tf\n","Iteration 7000, lr =  1.25e-06 loss: 1.3608431 acc: 0.908125\n","ERROR:tensorflow:Exception in QueueRunner: [_Derived_]RecvAsync is cancelled.\n","\t [[{{node DecodePng_2}}]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JvBZVns1TWWr"},"source":[""],"execution_count":null,"outputs":[]}]}